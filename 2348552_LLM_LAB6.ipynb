{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Embedding, LayerNormalization, Dropout\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(maxlen, embed_dim)\n",
    "\n",
    "    def get_angles(self, position, i, embed_dim):\n",
    "        angles = 1 / np.power(10000, (2 * (i // 2)) / np.float32(embed_dim))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, maxlen, embed_dim):\n",
    "        angle_rads = self.get_angles(np.arange(maxlen)[:, np.newaxis], np.arange(embed_dim)[np.newaxis, :], embed_dim)\n",
    "        sines = np.sin(angle_rads[:, 0::2])\n",
    "        cosines = np.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Self-Attention\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = Dense(embed_dim)\n",
    "        self.key_dense = Dense(embed_dim)\n",
    "        self.value_dense = Dense(embed_dim)\n",
    "        self.combine_heads = Dense(embed_dim)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "        attention, weights = self.scaled_dot_product_attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Block\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        attn_output = self.att(inputs, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, input_vocab_size, maxlen, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = Embedding(input_vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(maxlen, embed_dim)\n",
    "        self.enc_layers = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)]\n",
    "        self.dropout = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        inputs = self.embedding(inputs)\n",
    "        inputs *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "        inputs = self.pos_encoding(inputs)\n",
    "        inputs = self.dropout(inputs, training=training)\n",
    "        for layer in self.enc_layers:\n",
    "            inputs = layer(inputs, training=training)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder Layer\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, target_vocab_size, maxlen, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = Embedding(target_vocab_size, embed_dim)\n",
    "        self.pos_encoding = PositionalEncoding(maxlen, embed_dim)\n",
    "        self.dec_layers = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)]\n",
    "        self.dropout = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, enc_output, training=None):\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        inputs = self.embedding(inputs)\n",
    "        inputs *= tf.math.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "        inputs = self.pos_encoding(inputs)\n",
    "        inputs = self.dropout(inputs, training=training)\n",
    "        for layer in self.dec_layers:\n",
    "            inputs = layer(inputs, training=training)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Model\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, input_vocab_size, target_vocab_size, maxlen):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(num_layers, embed_dim, num_heads, ff_dim, input_vocab_size, maxlen)\n",
    "        self.decoder = Decoder(num_layers, embed_dim, num_heads, ff_dim, target_vocab_size, maxlen)\n",
    "        self.final_layer = Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        enc_input, dec_input = inputs\n",
    "        enc_output = self.encoder(enc_input, training=training)\n",
    "        dec_output = self.decoder(dec_input, enc_output, training=training)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "ff_dim = 512\n",
    "num_layers = 4\n",
    "input_vocab_size = 10000  # Example vocab size\n",
    "target_vocab_size = 10000  # Example vocab size\n",
    "maxlen = 100  # Maximum length of input/output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess the Dataset\n",
    "def load_data(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    pairs = [line.strip().split('\\t') for line in lines if len(line.split('\\t')) == 2]\n",
    "    return zip(*pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Model\n",
    "transformer = Transformer(num_layers, embed_dim, num_heads, ff_dim, input_vocab_size, target_vocab_size, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Model\n",
    "transformer.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Data Preparation\n",
    "def load_data(file_path):\n",
    "    input_texts, target_texts = [], []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            input_text, target_text = line.strip().split('\\t')\n",
    "            input_texts.append(input_text)\n",
    "            target_texts.append(target_text)\n",
    "    return input_texts, target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "input_texts, target_texts = load_data(r\"C:\\Users\\sanja\\Downloads\\french-english bilingual pairs\\fra.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "input_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "target_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "target_tokenizer.fit_on_texts(target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "target_sequences = target_tokenizer.texts_to_sequences(target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding\n",
    "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen=maxlen, padding='post')\n",
    "target_sequences = tf.keras.preprocessing.sequence.pad_sequences(target_sequences, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "split_idx = int(0.8 * len(input_sequences))\n",
    "input_train, input_val = input_sequences[:split_idx], input_sequences[split_idx:]\n",
    "target_train, target_val = target_sequences[:split_idx], target_sequences[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:664: UserWarning: Gradients do not exist for variables ['embeddings', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'gamma', 'beta', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'gamma', 'beta', 'gamma', 'beta'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node transformer_1/decoder_1/embedding_1_1/GatherV2 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\sanja\\AppData\\Local\\Temp\\ipykernel_26712\\1024614744.py\", line 2, in <module>\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 318, in fit\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 121, in one_step_on_iterator\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 108, in one_step_on_data\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 51, in train_step\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 882, in __call__\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\sanja\\AppData\\Local\\Temp\\ipykernel_26712\\3787975811.py\", line 12, in call\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 882, in __call__\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\sanja\\AppData\\Local\\Temp\\ipykernel_26712\\1748313623.py\", line 14, in call\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 882, in __call__\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 140, in call\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\numpy.py\", line 4875, in take\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\", line 1951, in take\n\nindices[17,1] = 15092 is not in [0, 10000)\n\t [[{{node transformer_1/decoder_1/embedding_1_1/GatherV2}}]] [Op:__inference_one_step_on_iterator_26663]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the Model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_val\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node transformer_1/decoder_1/embedding_1_1/GatherV2 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"C:\\Users\\sanja\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\sanja\\AppData\\Local\\Temp\\ipykernel_26712\\1024614744.py\", line 2, in <module>\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 318, in fit\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 121, in one_step_on_iterator\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 108, in one_step_on_data\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 51, in train_step\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 882, in __call__\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\sanja\\AppData\\Local\\Temp\\ipykernel_26712\\3787975811.py\", line 12, in call\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 882, in __call__\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\sanja\\AppData\\Local\\Temp\\ipykernel_26712\\1748313623.py\", line 14, in call\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py\", line 882, in __call__\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py\", line 46, in __call__\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 140, in call\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\numpy.py\", line 4875, in take\n\n  File \"c:\\Users\\sanja\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\", line 1951, in take\n\nindices[17,1] = 15092 is not in [0, 10000)\n\t [[{{node transformer_1/decoder_1/embedding_1_1/GatherV2}}]] [Op:__inference_one_step_on_iterator_26663]"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "transformer.fit(\n",
    "    [input_train, target_train[:, :-1]],\n",
    "    target_train[:, 1:],\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=([input_val, target_val[:, :-1]], target_val[:, 1:])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "transformer.save('transformer_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Function\n",
    "def predict_sentence(sentence, input_tokenizer, target_tokenizer, maxlen):\n",
    "    # Preprocess the input sentence\n",
    "    input_sequence = input_tokenizer.texts_to_sequences([sentence])\n",
    "    padded_input_sequence = tf.keras.preprocessing.sequence.pad_sequences(input_sequence, maxlen=maxlen, padding='post')\n",
    "    \n",
    "    # Predict\n",
    "    predictions = transformer.predict([padded_input_sequence, np.zeros((1, maxlen - 1))])\n",
    "    predicted_sequence = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    # Decode the sequence\n",
    "    decoded_sentence = ' '.join([target_tokenizer.index_word.get(idx, '') for idx in predicted_sequence[0]])\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model (if needed)\n",
    "transformer = tf.keras.models.load_model('transformer_model.h5', custom_objects={'Transformer': Transformer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of prediction\n",
    "input_sentence = \"Bonjour, comment Ã§a va?\"\n",
    "predicted_sentence = predict_sentence(input_sentence, input_tokenizer, target_tokenizer, maxlen)\n",
    "print(\"Predicted English Translation:\", predicted_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
